services:
  postgres:
    image: postgres:latest
    container_name: ocrbase-postgres
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: ocrbase
    volumes:
      - postgres_data:/var/lib/postgresql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d ocrbase"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:latest
    container_name: ocrbase-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  minio:
    image: minio/minio:latest
    container_name: ocrbase-minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    volumes:
      - minio_data:/data
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 10s
      timeout: 5s
      retries: 5

  # vLLM for local LLM inference (optional - requires GPU)
  # Uncomment to use local LLM instead of OpenRouter
  # vllm:
  #   image: vllm/vllm-openai:latest
  #   container_name: ocrbase-vllm
  #   ports:
  #     - "8000:8000"
  #   environment:
  #     - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN:-}
  #   volumes:
  #     - vllm_cache:/root/.cache/huggingface
  #   command: >
  #     --model meta-llama/Llama-3.3-70B-Instruct
  #     --tensor-parallel-size 1
  #     --max-model-len 8192
  #     --gpu-memory-utilization 0.9
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities: [gpu]
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5

volumes:
  postgres_data:
  redis_data:
  minio_data:
  # vllm_cache:
